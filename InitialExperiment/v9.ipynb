{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale and smote before 10-fold cross-validation for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0  org.apache.activemq.transport.amqp.AmqpFramePa...    4  10   1   9   2   5   \n",
      "1  org.apache.activemq.transport.amqp.AmqpHeader....    5   6   0   6   1   3   \n",
      "2  org.apache.activemq.transport.amqp.AmqpHeader....    1  13   0  13   3   9   \n",
      "3  org.apache.activemq.transport.amqp.AmqpInactiv...    1   5   0   5   1   3   \n",
      "4  org.apache.activemq.transport.amqp.AmqpInactiv...    6   9   0   9   1   5   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12       H13       H14   H15  H5  \\\n",
      "0   0    2    4  ...   1   2   2   4  0.200000  0.200000  1.000000   4.0   1   \n",
      "1   0    5    2  ...   1   1   3   4  0.333333  1.000000  0.333333   4.0   1   \n",
      "2   0    9    3  ...   1  10   0  10  0.769231  0.000000  0.000000  10.0   1   \n",
      "3   0    3    1  ...   1   2   0   2  0.400000  0.000000  0.000000   2.0   1   \n",
      "4   0    4    3  ...   1   3   2   5  0.333333  0.222222  1.500000   5.0   1   \n",
      "\n",
      "       bug-prone  \n",
      "0  not bug-prone  \n",
      "1  not bug-prone  \n",
      "2  not bug-prone  \n",
      "3  not bug-prone  \n",
      "4  not bug-prone  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Data/activemq_result.csv\", delimiter=',')\n",
    "# data = pd.read_csv(\"Data/avro_result.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data\n",
    "data_transform = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0  org.apache.activemq.transport.amqp.AmqpFramePa...    4  10   1   9   2   5   \n",
      "1  org.apache.activemq.transport.amqp.AmqpHeader....    5   6   0   6   1   3   \n",
      "2  org.apache.activemq.transport.amqp.AmqpHeader....    1  13   0  13   3   9   \n",
      "3  org.apache.activemq.transport.amqp.AmqpInactiv...    1   5   0   5   1   3   \n",
      "4  org.apache.activemq.transport.amqp.AmqpInactiv...    6   9   0   9   1   5   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12       H13       H14   H15  H5  \\\n",
      "0   0    2    4  ...   1   2   2   4  0.200000  0.200000  1.000000   4.0   1   \n",
      "1   0    5    2  ...   1   1   3   4  0.333333  1.000000  0.333333   4.0   1   \n",
      "2   0    9    3  ...   1  10   0  10  0.769231  0.000000  0.000000  10.0   1   \n",
      "3   0    3    1  ...   1   2   0   2  0.400000  0.000000  0.000000   2.0   1   \n",
      "4   0    4    3  ...   1   3   2   5  0.333333  0.222222  1.500000   5.0   1   \n",
      "\n",
      "   bug-prone  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'bug-prone' column to 0 and 1\n",
    "data_transform['bug-prone'] = data_transform['bug-prone'].apply(lambda x: 1 if x.strip() == 'bug-prone' else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data_transform.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4016, 42)\n"
     ]
    }
   ],
   "source": [
    "print(data_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split feature data and target data\n",
    "feature_X = data_transform.drop(columns=['Method name','bug-prone'])\n",
    "y = data_transform['bug-prone']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply SMOTETomek first (on original feature space)\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(feature_X, y)\n",
    "\n",
    "# Apply StandardScaler after resampling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X = X_scaled\n",
    "y = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4218, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "models = {\n",
    "    \n",
    "    \"RandomForest\" :  RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42),\n",
    "    \"AdaBoostClassifier\" : AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42),\n",
    "    \"BaggingClassifier\" : BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42),\n",
    "    \"KNeighborsClassifier\" : KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan'),\n",
    "    \"MLPClassifier\" : MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                               learning_rate='adaptive', random_state=42),\n",
    "    # \"GradientBoostingClassifier\" : GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    \"HistGradientBoostingClassifier\" : HistGradientBoostingClassifier(random_state=42),   \n",
    "    \"DecisionTreeClassifier\" : DecisionTreeClassifier(random_state=42),  # max_depth=10, min_samples_split=10, min_samples_leaf=5, \n",
    "    \"SVC\" : SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale'),\n",
    "    # \"GaussianNB\" : GaussianNB(var_smoothing=1e-9),\n",
    "    # \"LogisticRegression\" : LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    # \"LightGBM\": LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.8400 ± 0.0152\n",
      "Precision: 0.8330 ± 0.0328\n",
      "Recall: 0.8539 ± 0.0344\n",
      "F1-Score: 0.8422 ± 0.0133\n",
      "MCC: 0.6823 ± 0.0291\n",
      "AUC: 0.9182 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "    \n",
    "    # Average probabilities for small group\n",
    "    avg_proba = np.mean(probabilities, axis=0)\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, roc_auc_score, accuracy_score, precision_score,\n",
    "        recall_score, f1_score, matthews_corrcoef\n",
    "    )\n",
    "    \n",
    "\n",
    "    def get_optimal_threshold(y_true, y_proba):\n",
    "        \"\"\"\n",
    "        Compute the optimal threshold using Youden's Index.\n",
    "        \"\"\"\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "        youden_index = tpr - fpr\n",
    "        optimal_idx = np.argmax(youden_index)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        return optimal_threshold\n",
    "\n",
    "    best_threshold_pr = get_optimal_threshold(y_test, avg_proba)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (avg_proba > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, avg_proba))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "# Accuracy: 0.8569 ± 0.0339\n",
    "# Precision: 0.8623 ± 0.0439\n",
    "# Recall: 0.8527 ± 0.0641\n",
    "# F1-Score: 0.8555 ± 0.0372\n",
    "# MCC: 0.7172 ± 0.0670\n",
    "# AUC: 0.9186 ± 0.0274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.8352 ± 0.0170\n",
      "Precision: 0.8008 ± 0.0369\n",
      "Recall: 0.8981 ± 0.0393\n",
      "F1-Score: 0.8451 ± 0.0134\n",
      "MCC: 0.6788 ± 0.0293\n",
      "AUC: 0.9182 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "    \n",
    "    # Average probabilities for small group\n",
    "    avg_proba = np.mean(probabilities, axis=0)\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, roc_auc_score, accuracy_score, precision_score,\n",
    "        recall_score, f1_score, matthews_corrcoef\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, avg_proba)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (avg_proba > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, avg_proba))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "# Accuracy: 0.8555 ± 0.0366\n",
    "# Precision: 0.8422 ± 0.0603\n",
    "# Recall: 0.8847 ± 0.0624\n",
    "# F1-Score: 0.8596 ± 0.0330\n",
    "# MCC: 0.7183 ± 0.0677\n",
    "# AUC: 0.9186 ± 0.0274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.8369 ± 0.0171\n",
      "Precision: 0.8045 ± 0.0372\n",
      "Recall: 0.8952 ± 0.0358\n",
      "F1-Score: 0.8461 ± 0.0131\n",
      "MCC: 0.6811 ± 0.0302\n",
      "AUC: 0.9184 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "\n",
    "    combined_probs = np.sum([weight * probs for weight, probs in zip(weights, probabilities)], axis=0)\n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, combined_probs)\n",
    "    # adjusted_threshold = best_threshold_pr + 0.05\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8540 ± 0.0396\n",
    "# Precision: 0.8398 ± 0.0656\n",
    "# Recall: 0.8876 ± 0.0656\n",
    "# F1-Score: 0.8591 ± 0.0338\n",
    "# MCC: 0.7173 ± 0.0694\n",
    "# AUC: 0.9189 ± 0.0279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.8369 ± 0.0171\n",
      "Precision: 0.8045 ± 0.0372\n",
      "Recall: 0.8952 ± 0.0358\n",
      "F1-Score: 0.8461 ± 0.0131\n",
      "MCC: 0.6811 ± 0.0302\n",
      "AUC: 0.9184 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "    \n",
    "    # Compute weighted average probabilities\n",
    "    sigma_d = np.std(np.concatenate(probabilities))  # Compute standard deviation of probabilities\n",
    "    combined_probs = np.zeros_like(probabilities[0])\n",
    "\n",
    "    for weight, probs in zip(weights, probabilities):\n",
    "        max_proba = np.max(probs)  # Maximum probability from this model\n",
    "        if sigma_d < 0.25 and max_proba < 0.5:\n",
    "            combined_probs += weight * (1 - probs)  # Invert probabilities\n",
    "        else:\n",
    "            combined_probs += weight * probs\n",
    "\n",
    "    \n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, combined_probs)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8540 ± 0.0396\n",
    "# Precision: 0.8398 ± 0.0656\n",
    "# Recall: 0.8876 ± 0.0656\n",
    "# F1-Score: 0.8591 ± 0.0338\n",
    "# MCC: 0.7173 ± 0.0694\n",
    "# AUC: 0.9190 ± 0.0277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.4, Best F1: 0.8344\n",
      "Best threshold: 0.5100000000000001, Best F1: 0.8389\n",
      "Best threshold: 0.41000000000000003, Best F1: 0.8450\n",
      "Best threshold: 0.5000000000000001, Best F1: 0.8186\n",
      "Best threshold: 0.4800000000000001, Best F1: 0.8479\n",
      "Best threshold: 0.4, Best F1: 0.8486\n",
      "Best threshold: 0.5000000000000001, Best F1: 0.8645\n",
      "Best threshold: 0.5100000000000001, Best F1: 0.8659\n",
      "Best threshold: 0.5000000000000001, Best F1: 0.8585\n",
      "Best threshold: 0.4700000000000001, Best F1: 0.8453\n",
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.8386 ± 0.0176\n",
      "Precision: 0.8098 ± 0.0389\n",
      "Recall: 0.8909 ± 0.0405\n",
      "F1-Score: 0.8468 ± 0.0135\n",
      "MCC: 0.6841 ± 0.0304\n",
      "AUC: 0.9184 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "\n",
    "    combined_probs = np.sum([weight * probs for weight, probs in zip(weights, probabilities)], axis=0)\n",
    "\n",
    "\n",
    "    thresholds = np.arange(0.4, 0.6, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for t in thresholds:\n",
    "        preds = (combined_probs > t).astype(int)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    print(f\"Best threshold: {best_threshold}, Best F1: {best_f1:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8670 ± 0.0388\n",
    "# Precision: 0.8552 ± 0.0491\n",
    "# Recall: 0.8873 ± 0.0595\n",
    "# F1-Score: 0.8692 ± 0.0380\n",
    "# MCC: 0.7379 ± 0.0782\n",
    "# AUC: 0.9190 ± 0.0277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7630 ± 0.0435\n",
      "Precision: 0.7543 ± 0.0302\n",
      "Recall: 0.7801 ± 0.0946\n",
      "F1: 0.7645 ± 0.0516\n",
      "Mcc: 0.5306 ± 0.0913\n",
      "Auc: 0.7630 ± 0.0435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a Hard Voting Classifier\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        \n",
    "        (\"RandomForest\", RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42)),\n",
    "        (\"AdaBoostClassifier\", AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)),  \n",
    "        (\"BaggingClassifier\" , BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42)),\n",
    "        (\"KNeighborsClassifier\" , KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')),\n",
    "        (\"MLPClassifier\" , MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                                learning_rate='adaptive', random_state=42)),\n",
    "        # (\"GradientBoostingClassifier\" , GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)),\n",
    "        (\"HistGradientBoostingClassifier\" , HistGradientBoostingClassifier(random_state=42)),   \n",
    "        (\"DecisionTreeClassifier\" , DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)),\n",
    "        (\"SVC\" , SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale')),\n",
    "        # (\"GaussianNB\" , GaussianNB(var_smoothing=1e-9)),\n",
    "        # (\"LogisticRegression\" , LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        (\"XGBoost\", XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)),\n",
    "        (\"CatBoost\", CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42))\n",
    "    ],\n",
    "    voting='hard'  # Specify hard voting, where the majority class prediction is chosen\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, f1_score, \n",
    "    matthews_corrcoef, roc_auc_score\n",
    ")\n",
    "\n",
    "# Define custom scorers for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='binary'),  # For binary classification\n",
    "    'recall': make_scorer(recall_score, average='binary'),\n",
    "    'f1': make_scorer(f1_score, average='binary'),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'auc': make_scorer(roc_auc_score) # , needs_proba=True\n",
    "}\n",
    "\n",
    "# Helper function to display results\n",
    "def print_cv_results(results):\n",
    "    for metric in scoring.keys():\n",
    "        mean = results[f'test_{metric}'].mean()\n",
    "        std = results[f'test_{metric}'].std()\n",
    "        print(f\"{metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "def crossvalidate_fun(classifier, X_train, y_train):\n",
    "    cv_results = cross_validate(classifier, X_train, y_train, cv=10, scoring=scoring)\n",
    "    print_cv_results(cv_results)\n",
    "\n",
    "crossvalidate_fun(voting_clf_hard,X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7644 ± 0.0459\n",
      "Precision: 0.7369 ± 0.0356\n",
      "Recall: 0.8242 ± 0.0976\n",
      "F1: 0.7755 ± 0.0509\n",
      "Mcc: 0.5387 ± 0.1001\n",
      "Auc: 0.7644 ± 0.0459\n"
     ]
    }
   ],
   "source": [
    "# Create a Soft Voting Classifier\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        \n",
    "        (\"RandomForest\", RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42)),\n",
    "        (\"AdaBoostClassifier\", AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42)),  \n",
    "        (\"BaggingClassifier\" , BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42)),\n",
    "        (\"KNeighborsClassifier\" , KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')),\n",
    "        (\"MLPClassifier\" , MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                                learning_rate='adaptive', random_state=42)),\n",
    "        # (\"GradientBoostingClassifier\" , GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)),\n",
    "        (\"HistGradientBoostingClassifier\" , HistGradientBoostingClassifier(random_state=42)),   \n",
    "        (\"DecisionTreeClassifier\" , DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)),\n",
    "        (\"SVC\" , SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale')),\n",
    "        # (\"GaussianNB\" , GaussianNB(var_smoothing=1e-9)),\n",
    "        # (\"LogisticRegression\" , LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "        (\"XGBoost\", XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42)),\n",
    "        (\"CatBoost\", CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42))\n",
    "    ],\n",
    "    voting='soft'  # Specify soft voting, where class probabilities are combined\n",
    ")\n",
    "\n",
    "crossvalidate_fun(voting_clf_soft,X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
