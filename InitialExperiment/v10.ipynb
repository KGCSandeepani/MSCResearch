{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaler and smote apply during 10- fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0  org.apache.activemq.transport.amqp.AmqpFramePa...    4  10   1   9   2   5   \n",
      "1  org.apache.activemq.transport.amqp.AmqpHeader....    5   6   0   6   1   3   \n",
      "2  org.apache.activemq.transport.amqp.AmqpHeader....    1  13   0  13   3   9   \n",
      "3  org.apache.activemq.transport.amqp.AmqpInactiv...    1   5   0   5   1   3   \n",
      "4  org.apache.activemq.transport.amqp.AmqpInactiv...    6   9   0   9   1   5   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12       H13       H14   H15  H5  \\\n",
      "0   0    2    4  ...   1   2   2   4  0.200000  0.200000  1.000000   4.0   1   \n",
      "1   0    5    2  ...   1   1   3   4  0.333333  1.000000  0.333333   4.0   1   \n",
      "2   0    9    3  ...   1  10   0  10  0.769231  0.000000  0.000000  10.0   1   \n",
      "3   0    3    1  ...   1   2   0   2  0.400000  0.000000  0.000000   2.0   1   \n",
      "4   0    4    3  ...   1   3   2   5  0.333333  0.222222  1.500000   5.0   1   \n",
      "\n",
      "       bug-prone  \n",
      "0  not bug-prone  \n",
      "1  not bug-prone  \n",
      "2  not bug-prone  \n",
      "3  not bug-prone  \n",
      "4  not bug-prone  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Data/activemq_result.csv\", delimiter=',')\n",
    "# data = pd.read_csv(\"Data/cxf_result.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data\n",
    "data_transform = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0  org.apache.activemq.transport.amqp.AmqpFramePa...    4  10   1   9   2   5   \n",
      "1  org.apache.activemq.transport.amqp.AmqpHeader....    5   6   0   6   1   3   \n",
      "2  org.apache.activemq.transport.amqp.AmqpHeader....    1  13   0  13   3   9   \n",
      "3  org.apache.activemq.transport.amqp.AmqpInactiv...    1   5   0   5   1   3   \n",
      "4  org.apache.activemq.transport.amqp.AmqpInactiv...    6   9   0   9   1   5   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12       H13       H14   H15  H5  \\\n",
      "0   0    2    4  ...   1   2   2   4  0.200000  0.200000  1.000000   4.0   1   \n",
      "1   0    5    2  ...   1   1   3   4  0.333333  1.000000  0.333333   4.0   1   \n",
      "2   0    9    3  ...   1  10   0  10  0.769231  0.000000  0.000000  10.0   1   \n",
      "3   0    3    1  ...   1   2   0   2  0.400000  0.000000  0.000000   2.0   1   \n",
      "4   0    4    3  ...   1   3   2   5  0.333333  0.222222  1.500000   5.0   1   \n",
      "\n",
      "   bug-prone  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'bug-prone' column to 0 and 1\n",
    "data_transform['bug-prone'] = data_transform['bug-prone'].apply(lambda x: 1 if x.strip() == 'bug-prone' else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data_transform.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4016, 42)\n"
     ]
    }
   ],
   "source": [
    "print(data_transform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split feature data and target data\n",
    "feature_X = data_transform.drop(columns=['Method name','bug-prone'])\n",
    "y = data_transform['bug-prone']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply SMOTETomek first (on original feature space)\n",
    "smote = SMOTETomek(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(feature_X, y)\n",
    "\n",
    "# Apply StandardScaler after resampling\n",
    "scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# X = X_scaled\n",
    "# y = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42),\n",
    "    # \"AdaBoostClassifier\": AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42),  \n",
    "    \"BaggingClassifier\" : BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42),\n",
    "    \"KNeighborsClassifier\" : KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan'),\n",
    "    \"MLPClassifier\" : MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                               learning_rate='adaptive', random_state=42),\n",
    "    # \"GradientBoostingClassifier\" : GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    # \"HistGradientBoostingClassifier\" : HistGradientBoostingClassifier(random_state=42),   \n",
    "    # \"DecisionTreeClassifier\" : DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "    \"XGBClassifier\" : XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    # \"SVC\" : SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale'),\n",
    "    # \"GaussianNB\" : GaussianNB(var_smoothing=1e-9),\n",
    "    # \"LogisticRegression\" : LogisticRegression(class_weight='balanced', random_state=42),\n",
    "    # \"XGBoost\": XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    # \"LightGBM\": LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "# from imblearn.pipeline import Pipeline\n",
    "\n",
    "# models = {\n",
    "#     \"RandomForest\": Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "#                                        min_samples_leaf=2, class_weight='balanced', random_state=42))\n",
    "#     ]),\n",
    "#     \"AdaBoostClassifier\": Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42))\n",
    "#     ]),  \n",
    "#     \"BaggingClassifier\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42))]),\n",
    "#     \"KNeighborsClassifier\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan'))]),\n",
    "#     \"MLPClassifier\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "#                                learning_rate='adaptive', random_state=42))]),\n",
    "#     # \"GradientBoostingClassifier\" : GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "#     \"HistGradientBoostingClassifier\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', HistGradientBoostingClassifier(random_state=42))]),   \n",
    "#     # \"DecisionTreeClassifier\" : DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "#     \"XGBClassifier\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', XGBClassifier(eval_metric='logloss', random_state=42))]),\n",
    "#     # \"SVC\" : SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale'),\n",
    "#     # \"GaussianNB\" : GaussianNB(var_smoothing=1e-9),\n",
    "#     \"LogisticRegression\" : Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', LogisticRegression(class_weight='balanced', random_state=42))]),\n",
    "#     \"XGBoost\": Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42))]),\n",
    "#     # \"LightGBM\": LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "#     \"CatBoost\": Pipeline([\n",
    "#         ('smote', SMOTETomek(random_state=42)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model', CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42))])\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 (RandomForest): 0.7209\n",
      "Acc (RandomForest): 0.7612\n",
      "\n",
      "F1 (BaggingClassifier): 0.7536\n",
      "Acc (BaggingClassifier): 0.7886\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6486\n",
      "Acc (KNeighborsClassifier): 0.7090\n",
      "\n",
      "F1 (MLPClassifier): 0.7011\n",
      "Acc (MLPClassifier): 0.7413\n",
      "\n",
      "F1 (XGBClassifier): 0.7310\n",
      "Acc (XGBClassifier): 0.7711\n",
      "\n",
      "F1 (CatBoost): 0.6822\n",
      "Acc (CatBoost): 0.7289\n",
      "\n",
      "F1 (RandomForest): 0.7768\n",
      "Acc (RandomForest): 0.8085\n",
      "\n",
      "F1 (BaggingClassifier): 0.7666\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7294\n",
      "Acc (KNeighborsClassifier): 0.7711\n",
      "\n",
      "F1 (MLPClassifier): 0.6997\n",
      "Acc (MLPClassifier): 0.7438\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7736\n",
      "\n",
      "F1 (CatBoost): 0.7256\n",
      "Acc (CatBoost): 0.7761\n",
      "\n",
      "F1 (RandomForest): 0.7944\n",
      "Acc (RandomForest): 0.8159\n",
      "\n",
      "F1 (BaggingClassifier): 0.7744\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6852\n",
      "Acc (KNeighborsClassifier): 0.7189\n",
      "\n",
      "F1 (MLPClassifier): 0.6755\n",
      "Acc (MLPClassifier): 0.6940\n",
      "\n",
      "F1 (XGBClassifier): 0.7543\n",
      "Acc (XGBClassifier): 0.7861\n",
      "\n",
      "F1 (CatBoost): 0.7110\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7200\n",
      "Acc (RandomForest): 0.7562\n",
      "\n",
      "F1 (BaggingClassifier): 0.7283\n",
      "Acc (BaggingClassifier): 0.7662\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7224\n",
      "Acc (KNeighborsClassifier): 0.7687\n",
      "\n",
      "F1 (MLPClassifier): 0.6792\n",
      "Acc (MLPClassifier): 0.7463\n",
      "\n",
      "F1 (XGBClassifier): 0.7126\n",
      "Acc (XGBClassifier): 0.7512\n",
      "\n",
      "F1 (CatBoost): 0.6585\n",
      "Acc (CatBoost): 0.7214\n",
      "\n",
      "F1 (RandomForest): 0.7371\n",
      "Acc (RandomForest): 0.7711\n",
      "\n",
      "F1 (BaggingClassifier): 0.7339\n",
      "Acc (BaggingClassifier): 0.7637\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7143\n",
      "Acc (KNeighborsClassifier): 0.7612\n",
      "\n",
      "F1 (MLPClassifier): 0.7047\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7662\n",
      "\n",
      "F1 (CatBoost): 0.6933\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7357\n",
      "Acc (RandomForest): 0.7587\n",
      "\n",
      "F1 (BaggingClassifier): 0.7328\n",
      "Acc (BaggingClassifier): 0.7587\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7514\n",
      "Acc (KNeighborsClassifier): 0.7811\n",
      "\n",
      "F1 (MLPClassifier): 0.6816\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7293\n",
      "Acc (XGBClassifier): 0.7562\n",
      "\n",
      "F1 (CatBoost): 0.7218\n",
      "Acc (CatBoost): 0.7488\n",
      "\n",
      "F1 (RandomForest): 0.7359\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7399\n",
      "Acc (BaggingClassifier): 0.7756\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7066\n",
      "Acc (KNeighborsClassifier): 0.7556\n",
      "\n",
      "F1 (MLPClassifier): 0.6667\n",
      "Acc (MLPClassifier): 0.7007\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6768\n",
      "Acc (CatBoost): 0.7357\n",
      "\n",
      "F1 (RandomForest): 0.7087\n",
      "Acc (RandomForest): 0.7581\n",
      "\n",
      "F1 (BaggingClassifier): 0.7101\n",
      "Acc (BaggingClassifier): 0.7556\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6848\n",
      "Acc (KNeighborsClassifier): 0.7406\n",
      "\n",
      "F1 (MLPClassifier): 0.6807\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6182\n",
      "Acc (CatBoost): 0.6858\n",
      "\n",
      "F1 (RandomForest): 0.7069\n",
      "Acc (RandomForest): 0.7456\n",
      "\n",
      "F1 (BaggingClassifier): 0.7069\n",
      "Acc (BaggingClassifier): 0.7456\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6453\n",
      "Acc (KNeighborsClassifier): 0.6958\n",
      "\n",
      "F1 (MLPClassifier): 0.7123\n",
      "Acc (MLPClassifier): 0.7481\n",
      "\n",
      "F1 (XGBClassifier): 0.6901\n",
      "Acc (XGBClassifier): 0.7357\n",
      "\n",
      "F1 (CatBoost): 0.6647\n",
      "Acc (CatBoost): 0.7207\n",
      "\n",
      "F1 (RandomForest): 0.7390\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7294\n",
      "Acc (BaggingClassifier): 0.7706\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7283\n",
      "Acc (KNeighborsClassifier): 0.7656\n",
      "\n",
      "F1 (MLPClassifier): 0.6687\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7311\n",
      "Acc (XGBClassifier): 0.7781\n",
      "\n",
      "F1 (CatBoost): 0.6914\n",
      "Acc (CatBoost): 0.7506\n",
      "\n",
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.7799 ± 0.0176\n",
      "Precision: 0.7126 ± 0.0447\n",
      "Recall: 0.8101 ± 0.0644\n",
      "F1-Score: 0.7546 ± 0.0135\n",
      "MCC: 0.5660 ± 0.0234\n",
      "AUC: 0.8705 ± 0.0124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(feature_X, y):\n",
    "    X_train, X_test = feature_X.iloc[train_index], feature_X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTETomek and StandardScaler inside each fold\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_resampled)       \n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "        print(f\"\\nF1 ({name}): {f1_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "        print(f\"Acc ({name}): {accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "\n",
    "    \n",
    "    # Average probabilities for small group\n",
    "    avg_proba = np.mean(probabilities, axis=0)\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, roc_auc_score, accuracy_score, precision_score,\n",
    "        recall_score, f1_score, matthews_corrcoef\n",
    "    )\n",
    "    \n",
    "\n",
    "    def get_optimal_threshold(y_true, y_proba):\n",
    "        \"\"\"\n",
    "        Compute the optimal threshold using Youden's Index.\n",
    "        \"\"\"\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "        youden_index = tpr - fpr\n",
    "        optimal_idx = np.argmax(youden_index)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        return optimal_threshold\n",
    "\n",
    "    best_threshold_pr = get_optimal_threshold(y_test, avg_proba)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (avg_proba > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, avg_proba))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"\\nFinal 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "# Accuracy: 0.8569 ± 0.0339\n",
    "# Precision: 0.8623 ± 0.0439\n",
    "# Recall: 0.8527 ± 0.0641\n",
    "# F1-Score: 0.8555 ± 0.0372\n",
    "# MCC: 0.7172 ± 0.0670\n",
    "# AUC: 0.9186 ± 0.0274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 (RandomForest): 0.7209\n",
      "Acc (RandomForest): 0.7612\n",
      "\n",
      "F1 (BaggingClassifier): 0.7536\n",
      "Acc (BaggingClassifier): 0.7886\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6486\n",
      "Acc (KNeighborsClassifier): 0.7090\n",
      "\n",
      "F1 (MLPClassifier): 0.7011\n",
      "Acc (MLPClassifier): 0.7413\n",
      "\n",
      "F1 (XGBClassifier): 0.7310\n",
      "Acc (XGBClassifier): 0.7711\n",
      "\n",
      "F1 (CatBoost): 0.6822\n",
      "Acc (CatBoost): 0.7289\n",
      "\n",
      "F1 (RandomForest): 0.7768\n",
      "Acc (RandomForest): 0.8085\n",
      "\n",
      "F1 (BaggingClassifier): 0.7666\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7294\n",
      "Acc (KNeighborsClassifier): 0.7711\n",
      "\n",
      "F1 (MLPClassifier): 0.6997\n",
      "Acc (MLPClassifier): 0.7438\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7736\n",
      "\n",
      "F1 (CatBoost): 0.7256\n",
      "Acc (CatBoost): 0.7761\n",
      "\n",
      "F1 (RandomForest): 0.7944\n",
      "Acc (RandomForest): 0.8159\n",
      "\n",
      "F1 (BaggingClassifier): 0.7744\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6852\n",
      "Acc (KNeighborsClassifier): 0.7189\n",
      "\n",
      "F1 (MLPClassifier): 0.6755\n",
      "Acc (MLPClassifier): 0.6940\n",
      "\n",
      "F1 (XGBClassifier): 0.7543\n",
      "Acc (XGBClassifier): 0.7861\n",
      "\n",
      "F1 (CatBoost): 0.7110\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7200\n",
      "Acc (RandomForest): 0.7562\n",
      "\n",
      "F1 (BaggingClassifier): 0.7283\n",
      "Acc (BaggingClassifier): 0.7662\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7224\n",
      "Acc (KNeighborsClassifier): 0.7687\n",
      "\n",
      "F1 (MLPClassifier): 0.6792\n",
      "Acc (MLPClassifier): 0.7463\n",
      "\n",
      "F1 (XGBClassifier): 0.7126\n",
      "Acc (XGBClassifier): 0.7512\n",
      "\n",
      "F1 (CatBoost): 0.6585\n",
      "Acc (CatBoost): 0.7214\n",
      "\n",
      "F1 (RandomForest): 0.7371\n",
      "Acc (RandomForest): 0.7711\n",
      "\n",
      "F1 (BaggingClassifier): 0.7339\n",
      "Acc (BaggingClassifier): 0.7637\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7143\n",
      "Acc (KNeighborsClassifier): 0.7612\n",
      "\n",
      "F1 (MLPClassifier): 0.7047\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7662\n",
      "\n",
      "F1 (CatBoost): 0.6933\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7357\n",
      "Acc (RandomForest): 0.7587\n",
      "\n",
      "F1 (BaggingClassifier): 0.7328\n",
      "Acc (BaggingClassifier): 0.7587\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7514\n",
      "Acc (KNeighborsClassifier): 0.7811\n",
      "\n",
      "F1 (MLPClassifier): 0.6816\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7293\n",
      "Acc (XGBClassifier): 0.7562\n",
      "\n",
      "F1 (CatBoost): 0.7218\n",
      "Acc (CatBoost): 0.7488\n",
      "\n",
      "F1 (RandomForest): 0.7359\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7399\n",
      "Acc (BaggingClassifier): 0.7756\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7066\n",
      "Acc (KNeighborsClassifier): 0.7556\n",
      "\n",
      "F1 (MLPClassifier): 0.6667\n",
      "Acc (MLPClassifier): 0.7007\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6768\n",
      "Acc (CatBoost): 0.7357\n",
      "\n",
      "F1 (RandomForest): 0.7087\n",
      "Acc (RandomForest): 0.7581\n",
      "\n",
      "F1 (BaggingClassifier): 0.7101\n",
      "Acc (BaggingClassifier): 0.7556\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6848\n",
      "Acc (KNeighborsClassifier): 0.7406\n",
      "\n",
      "F1 (MLPClassifier): 0.6807\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6182\n",
      "Acc (CatBoost): 0.6858\n",
      "\n",
      "F1 (RandomForest): 0.7069\n",
      "Acc (RandomForest): 0.7456\n",
      "\n",
      "F1 (BaggingClassifier): 0.7069\n",
      "Acc (BaggingClassifier): 0.7456\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6453\n",
      "Acc (KNeighborsClassifier): 0.6958\n",
      "\n",
      "F1 (MLPClassifier): 0.7123\n",
      "Acc (MLPClassifier): 0.7481\n",
      "\n",
      "F1 (XGBClassifier): 0.6901\n",
      "Acc (XGBClassifier): 0.7357\n",
      "\n",
      "F1 (CatBoost): 0.6647\n",
      "Acc (CatBoost): 0.7207\n",
      "\n",
      "F1 (RandomForest): 0.7390\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7294\n",
      "Acc (BaggingClassifier): 0.7706\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7283\n",
      "Acc (KNeighborsClassifier): 0.7656\n",
      "\n",
      "F1 (MLPClassifier): 0.6687\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7311\n",
      "Acc (XGBClassifier): 0.7781\n",
      "\n",
      "F1 (CatBoost): 0.6914\n",
      "Acc (CatBoost): 0.7506\n",
      "\n",
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.7652 ± 0.0191\n",
      "Precision: 0.6690 ± 0.0320\n",
      "Recall: 0.8798 ± 0.0454\n",
      "F1-Score: 0.7583 ± 0.0111\n",
      "MCC: 0.5612 ± 0.0227\n",
      "AUC: 0.8705 ± 0.0124\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(feature_X, y):\n",
    "    X_train, X_test = feature_X.iloc[train_index], feature_X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTETomek and StandardScaler inside each fold\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "        print(f\"\\nF1 ({name}): {f1_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "        print(f\"Acc ({name}): {accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "    \n",
    "    # Average probabilities for small group\n",
    "    avg_proba = np.mean(probabilities, axis=0)\n",
    "\n",
    "    from sklearn.metrics import (\n",
    "        roc_curve, roc_auc_score, accuracy_score, precision_score,\n",
    "        recall_score, f1_score, matthews_corrcoef\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, avg_proba)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (avg_proba > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, avg_proba))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"\\nFinal 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "# Accuracy: 0.8555 ± 0.0366\n",
    "# Precision: 0.8422 ± 0.0603\n",
    "# Recall: 0.8847 ± 0.0624\n",
    "# F1-Score: 0.8596 ± 0.0330\n",
    "# MCC: 0.7183 ± 0.0677\n",
    "# AUC: 0.9186 ± 0.0274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 (RandomForest): 0.7209\n",
      "Acc (RandomForest): 0.7612\n",
      "\n",
      "F1 (BaggingClassifier): 0.7536\n",
      "Acc (BaggingClassifier): 0.7886\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6486\n",
      "Acc (KNeighborsClassifier): 0.7090\n",
      "\n",
      "F1 (MLPClassifier): 0.7011\n",
      "Acc (MLPClassifier): 0.7413\n",
      "\n",
      "F1 (XGBClassifier): 0.7310\n",
      "Acc (XGBClassifier): 0.7711\n",
      "\n",
      "F1 (CatBoost): 0.6822\n",
      "Acc (CatBoost): 0.7289\n",
      "\n",
      "F1 (RandomForest): 0.7768\n",
      "Acc (RandomForest): 0.8085\n",
      "\n",
      "F1 (BaggingClassifier): 0.7666\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7294\n",
      "Acc (KNeighborsClassifier): 0.7711\n",
      "\n",
      "F1 (MLPClassifier): 0.6997\n",
      "Acc (MLPClassifier): 0.7438\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7736\n",
      "\n",
      "F1 (CatBoost): 0.7256\n",
      "Acc (CatBoost): 0.7761\n",
      "\n",
      "F1 (RandomForest): 0.7944\n",
      "Acc (RandomForest): 0.8159\n",
      "\n",
      "F1 (BaggingClassifier): 0.7744\n",
      "Acc (BaggingClassifier): 0.7985\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6852\n",
      "Acc (KNeighborsClassifier): 0.7189\n",
      "\n",
      "F1 (MLPClassifier): 0.6755\n",
      "Acc (MLPClassifier): 0.6940\n",
      "\n",
      "F1 (XGBClassifier): 0.7543\n",
      "Acc (XGBClassifier): 0.7861\n",
      "\n",
      "F1 (CatBoost): 0.7110\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7200\n",
      "Acc (RandomForest): 0.7562\n",
      "\n",
      "F1 (BaggingClassifier): 0.7283\n",
      "Acc (BaggingClassifier): 0.7662\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7224\n",
      "Acc (KNeighborsClassifier): 0.7687\n",
      "\n",
      "F1 (MLPClassifier): 0.6792\n",
      "Acc (MLPClassifier): 0.7463\n",
      "\n",
      "F1 (XGBClassifier): 0.7126\n",
      "Acc (XGBClassifier): 0.7512\n",
      "\n",
      "F1 (CatBoost): 0.6585\n",
      "Acc (CatBoost): 0.7214\n",
      "\n",
      "F1 (RandomForest): 0.7371\n",
      "Acc (RandomForest): 0.7711\n",
      "\n",
      "F1 (BaggingClassifier): 0.7339\n",
      "Acc (BaggingClassifier): 0.7637\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7143\n",
      "Acc (KNeighborsClassifier): 0.7612\n",
      "\n",
      "F1 (MLPClassifier): 0.7047\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7251\n",
      "Acc (XGBClassifier): 0.7662\n",
      "\n",
      "F1 (CatBoost): 0.6933\n",
      "Acc (CatBoost): 0.7512\n",
      "\n",
      "F1 (RandomForest): 0.7357\n",
      "Acc (RandomForest): 0.7587\n",
      "\n",
      "F1 (BaggingClassifier): 0.7328\n",
      "Acc (BaggingClassifier): 0.7587\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7514\n",
      "Acc (KNeighborsClassifier): 0.7811\n",
      "\n",
      "F1 (MLPClassifier): 0.6816\n",
      "Acc (MLPClassifier): 0.7164\n",
      "\n",
      "F1 (XGBClassifier): 0.7293\n",
      "Acc (XGBClassifier): 0.7562\n",
      "\n",
      "F1 (CatBoost): 0.7218\n",
      "Acc (CatBoost): 0.7488\n",
      "\n",
      "F1 (RandomForest): 0.7359\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7399\n",
      "Acc (BaggingClassifier): 0.7756\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7066\n",
      "Acc (KNeighborsClassifier): 0.7556\n",
      "\n",
      "F1 (MLPClassifier): 0.6667\n",
      "Acc (MLPClassifier): 0.7007\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6768\n",
      "Acc (CatBoost): 0.7357\n",
      "\n",
      "F1 (RandomForest): 0.7087\n",
      "Acc (RandomForest): 0.7581\n",
      "\n",
      "F1 (BaggingClassifier): 0.7101\n",
      "Acc (BaggingClassifier): 0.7556\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6848\n",
      "Acc (KNeighborsClassifier): 0.7406\n",
      "\n",
      "F1 (MLPClassifier): 0.6807\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7087\n",
      "Acc (XGBClassifier): 0.7581\n",
      "\n",
      "F1 (CatBoost): 0.6182\n",
      "Acc (CatBoost): 0.6858\n",
      "\n",
      "F1 (RandomForest): 0.7069\n",
      "Acc (RandomForest): 0.7456\n",
      "\n",
      "F1 (BaggingClassifier): 0.7069\n",
      "Acc (BaggingClassifier): 0.7456\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.6453\n",
      "Acc (KNeighborsClassifier): 0.6958\n",
      "\n",
      "F1 (MLPClassifier): 0.7123\n",
      "Acc (MLPClassifier): 0.7481\n",
      "\n",
      "F1 (XGBClassifier): 0.6901\n",
      "Acc (XGBClassifier): 0.7357\n",
      "\n",
      "F1 (CatBoost): 0.6647\n",
      "Acc (CatBoost): 0.7207\n",
      "\n",
      "F1 (RandomForest): 0.7390\n",
      "Acc (RandomForest): 0.7781\n",
      "\n",
      "F1 (BaggingClassifier): 0.7294\n",
      "Acc (BaggingClassifier): 0.7706\n",
      "\n",
      "F1 (KNeighborsClassifier): 0.7283\n",
      "Acc (KNeighborsClassifier): 0.7656\n",
      "\n",
      "F1 (MLPClassifier): 0.6687\n",
      "Acc (MLPClassifier): 0.7357\n",
      "\n",
      "F1 (XGBClassifier): 0.7311\n",
      "Acc (XGBClassifier): 0.7781\n",
      "\n",
      "F1 (CatBoost): 0.6914\n",
      "Acc (CatBoost): 0.7506\n",
      "\n",
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.7679 ± 0.0188\n",
      "Precision: 0.6745 ± 0.0315\n",
      "Recall: 0.8702 ± 0.0389\n",
      "F1-Score: 0.7585 ± 0.0117\n",
      "MCC: 0.5613 ± 0.0242\n",
      "AUC: 0.8708 ± 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(feature_X, y):\n",
    "    X_train, X_test = feature_X.iloc[train_index], feature_X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTETomek and StandardScaler inside each fold\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "        print(f\"\\nF1 ({name}): {f1_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "        print(f\"Acc ({name}): {accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "\n",
    "    combined_probs = np.sum([weight * probs for weight, probs in zip(weights, probabilities)], axis=0)\n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, combined_probs)\n",
    "    # adjusted_threshold = best_threshold_pr + 0.05\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"\\nFinal 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8540 ± 0.0396\n",
    "# Precision: 0.8398 ± 0.0656\n",
    "# Recall: 0.8876 ± 0.0656\n",
    "# F1-Score: 0.8591 ± 0.0338\n",
    "# MCC: 0.7173 ± 0.0694\n",
    "# AUC: 0.9189 ± 0.0279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.7679 ± 0.0188\n",
      "Precision: 0.6745 ± 0.0315\n",
      "Recall: 0.8702 ± 0.0389\n",
      "F1-Score: 0.7585 ± 0.0117\n",
      "MCC: 0.5613 ± 0.0242\n",
      "AUC: 0.8708 ± 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(feature_X, y):\n",
    "    X_train, X_test = feature_X.iloc[train_index], feature_X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTETomek and StandardScaler inside each fold\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "        # print(f\"F1 ({name}): {f1_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "        # print(f\"Acc ({name}): {accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "    \n",
    "    # Compute weighted average probabilities\n",
    "    sigma_d = np.std(np.concatenate(probabilities))  # Compute standard deviation of probabilities\n",
    "    combined_probs = np.zeros_like(probabilities[0])\n",
    "\n",
    "    for weight, probs in zip(weights, probabilities):\n",
    "        max_proba = np.max(probs)  # Maximum probability from this model\n",
    "        if sigma_d < 0.25 and max_proba < 0.5:\n",
    "            combined_probs += weight * (1 - probs)  # Invert probabilities\n",
    "        else:\n",
    "            combined_probs += weight * probs\n",
    "\n",
    "    \n",
    "\n",
    "    # Determine best threshold using precision-recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "        # f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "        # f1_scores = np.nan_to_num(f1_scores)\n",
    "        f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        return best_threshold\n",
    "\n",
    "    best_threshold_pr = get_best_threshold_precision_recall(y_test, combined_probs)\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8540 ± 0.0396\n",
    "# Precision: 0.8398 ± 0.0656\n",
    "# Recall: 0.8876 ± 0.0656\n",
    "# F1-Score: 0.8591 ± 0.0338\n",
    "# MCC: 0.7173 ± 0.0694\n",
    "# AUC: 0.9190 ± 0.0277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.43000000000000005, Best F1: 0.7598\n",
      "Best threshold: 0.45000000000000007, Best F1: 0.7857\n",
      "Best threshold: 0.41000000000000003, Best F1: 0.7617\n",
      "Best threshold: 0.4, Best F1: 0.7560\n",
      "Best threshold: 0.4600000000000001, Best F1: 0.7493\n",
      "Best threshold: 0.4, Best F1: 0.7551\n",
      "Best threshold: 0.42000000000000004, Best F1: 0.7658\n",
      "Best threshold: 0.4700000000000001, Best F1: 0.7449\n",
      "Best threshold: 0.5300000000000001, Best F1: 0.7235\n",
      "Best threshold: 0.4, Best F1: 0.7493\n",
      "Final 10-Fold CV Results:\n",
      "Accuracy: 0.7771 ± 0.0129\n",
      "Precision: 0.7000 ± 0.0235\n",
      "Recall: 0.8226 ± 0.0448\n",
      "F1-Score: 0.7551 ± 0.0151\n",
      "MCC: 0.5612 ± 0.0240\n",
      "AUC: 0.8708 ± 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores, mcc_scores, auc_scores = [], [], [], [], [], []\n",
    "\n",
    "# Perform 10-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(feature_X, y):\n",
    "    X_train, X_test = feature_X.iloc[train_index], feature_X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Apply SMOTETomek and StandardScaler inside each fold\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    probabilities = []\n",
    "    max_scores = []\n",
    "    \n",
    "    # Train all models and get probabilities\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train_resampled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
    "        probabilities.append(y_pred_proba)\n",
    "        max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "        # print(f\"F1 ({name}): {f1_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "        # print(f\"Acc ({name}): {accuracy_score(y_test, (y_pred_proba > 0.5).astype(int)):.4f}\")\n",
    "    \n",
    "    # Normalize AUC scores to get weights\n",
    "    weights = [score / sum(max_scores) for score in max_scores]\n",
    "\n",
    "    combined_probs = np.sum([weight * probs for weight, probs in zip(weights, probabilities)], axis=0)\n",
    "\n",
    "\n",
    "    thresholds = np.arange(0.4, 0.6, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for t in thresholds:\n",
    "        preds = (combined_probs > t).astype(int)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = t\n",
    "    print(f\"Best threshold: {best_threshold}, Best F1: {best_f1:.4f}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    predictions = (combined_probs > best_threshold).astype(int)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "    precision_scores.append(precision_score(y_test, predictions))\n",
    "    recall_scores.append(recall_score(y_test, predictions))\n",
    "    f1_scores.append(f1_score(y_test, predictions))\n",
    "    mcc_scores.append(matthews_corrcoef(y_test, predictions))\n",
    "    auc_scores.append(roc_auc_score(y_test, combined_probs))\n",
    "\n",
    "# Print final results (average over 10 folds)\n",
    "print(f\"Final 10-Fold CV Results:\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
    "print(f\"Precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
    "print(f\"Recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
    "print(f\"F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"MCC: {np.mean(mcc_scores):.4f} ± {np.std(mcc_scores):.4f}\")\n",
    "print(f\"AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")\n",
    "\n",
    "\n",
    "# Accuracy: 0.8670 ± 0.0388\n",
    "# Precision: 0.8552 ± 0.0491\n",
    "# Recall: 0.8873 ± 0.0595\n",
    "# F1-Score: 0.8692 ± 0.0380\n",
    "# MCC: 0.7379 ± 0.0782\n",
    "# AUC: 0.9190 ± 0.0277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Fit the model\n",
    "# rf_classifier_s.fit(X_train_resampled_s, y_train_resampled_s)\n",
    "\n",
    "# # Explain predictions\n",
    "# explainer = shap.TreeExplainer(rf_classifier_s)\n",
    "# shap_values = explainer.shap_values(X_test_resampled_s)\n",
    "\n",
    "# # Visualize feature importance\n",
    "# shap.summary_plot(shap_values[1], X_test_resampled_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "\n",
    "# # Create three different classifiers\n",
    "# clf1 = LogisticRegression(random_state=1)\n",
    "# clf2 = RandomForestClassifier(random_state=1)\n",
    "# clf3 = GaussianNB()\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of labels for the classifiers\n",
    "# labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "\n",
    "# Create a Hard Voting Classifier\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        \n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42)),\n",
    "        # \"AdaBoostClassifier\", AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42),  \n",
    "        (\"BaggingClassifier\" , BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42)),\n",
    "        (\"KNeighborsClassifier\" , KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan')),\n",
    "        (\"MLPClassifier\" , MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                                learning_rate='adaptive', random_state=42)),\n",
    "        # \"GradientBoostingClassifier\" , GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "        # \"HistGradientBoostingClassifier\" , HistGradientBoostingClassifier(random_state=42),   \n",
    "        # \"DecisionTreeClassifier\" , DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42),\n",
    "        # \"SVC\" , SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale'),\n",
    "        # \"GaussianNB\" , GaussianNB(var_smoothing=1e-9),\n",
    "        # \"LogisticRegression\" , LogisticRegression(class_weight='balanced', random_state=42),\n",
    "        # \"XGBoost\", XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "        # \"LightGBM\", LGBMClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "        (\"CatBoost\", CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42))\n",
    "    ],\n",
    "    voting='hard'  # Specify hard voting, where the majority class prediction is chosen\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, precision_score, recall_score, f1_score, \n",
    "    matthews_corrcoef, roc_auc_score\n",
    ")\n",
    "\n",
    "# Define custom scorers for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='binary'),  # For binary classification\n",
    "    'recall': make_scorer(recall_score, average='binary'),\n",
    "    'f1': make_scorer(f1_score, average='binary'),\n",
    "    'mcc': make_scorer(matthews_corrcoef),\n",
    "    'auc': make_scorer(roc_auc_score) # , needs_proba=True\n",
    "}\n",
    "\n",
    "# Helper function to display results\n",
    "def print_cv_results(results):\n",
    "    for metric in scoring.keys():\n",
    "        mean = results[f'test_{metric}'].mean()\n",
    "        std = results[f'test_{metric}'].std()\n",
    "        print(f\"{metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "def crossvalidate_fun(classifier, X_train, y_train):\n",
    "    cv_results = cross_validate(classifier, X_train, y_train, cv=10, scoring=scoring)\n",
    "    print_cv_results(cv_results)\n",
    "\n",
    "# crossvalidate_fun(voting_clf_hard,feature_X, y)\n",
    "\n",
    "# voting_clf_hard = voting_clf_hard.fit(feature_X, y)\n",
    "# print(voting_clf_hard.predict(feature_X))\n",
    "\n",
    "# Create a Soft Voting Classifier\n",
    "# voting_clf_soft = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         (labels[0], clf1),  # Include the first classifier (Logistic Regression)\n",
    "#         (labels[1], clf2),  # Include the second classifier (Random Forest)\n",
    "#         (labels[2], clf3),  # Include the third classifier (Naive Bayes)\n",
    "#     ],\n",
    "#     voting='soft'  # Specify soft voting, where class probabilities are combined\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
