{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import pandas libraries\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset\n",
    "# data = pd.read_csv(\"Data/activemq_result.csv\", delimiter=',')\n",
    "# # data = pd.read_csv(\"Data/avro_result.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "# # Explore the dataset\n",
    "# print(data.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4016, 42)\n",
      "(619, 42)\n",
      "(965, 42)\n",
      "(12081, 42)\n",
      "(1883, 42)\n",
      "(6415, 42)\n",
      "(2187, 42)\n",
      "(2438, 42)\n",
      "(473, 42)\n",
      "(1873, 42)\n",
      "(2238, 42)\n",
      "(908, 42)\n",
      "(1167, 42)\n",
      "(629, 42)\n",
      "(3936, 42)\n",
      "(41828, 42)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to the directory containing your CSV files\n",
    "file_path = \"Train/*.csv\"  # Use a wildcard to match all CSV files in the directory\n",
    "\n",
    "# Get all file paths matching the pattern\n",
    "all_files = glob.glob(file_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "data_list = []\n",
    "\n",
    "# Read and combine files\n",
    "for i, file in enumerate(all_files):\n",
    "    if i == 0:\n",
    "        # Include header only for the first file\n",
    "        df = pd.read_csv(file, delimiter=',')\n",
    "        print(df.shape)\n",
    "    else:\n",
    "        # Skip the header row for subsequent files\n",
    "        df = pd.read_csv(file, delimiter=',', header=0)\n",
    "        print(df.shape)\n",
    "    data_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_data = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "data = combined_data.copy()\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data\n",
    "data_transform = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0  org.apache.activemq.transport.amqp.AmqpFramePa...    4  10   1   9   2   5   \n",
      "1  org.apache.activemq.transport.amqp.AmqpHeader....    5   6   0   6   1   3   \n",
      "2  org.apache.activemq.transport.amqp.AmqpHeader....    1  13   0  13   3   9   \n",
      "3  org.apache.activemq.transport.amqp.AmqpInactiv...    1   5   0   5   1   3   \n",
      "4  org.apache.activemq.transport.amqp.AmqpInactiv...    6   9   0   9   1   5   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12       H13       H14   H15  H5  \\\n",
      "0   0    2    4  ...   1   2   2   4  0.200000  0.200000  1.000000   4.0   1   \n",
      "1   0    5    2  ...   1   1   3   4  0.333333  1.000000  0.333333   4.0   1   \n",
      "2   0    9    3  ...   1  10   0  10  0.769231  0.000000  0.000000  10.0   1   \n",
      "3   0    3    1  ...   1   2   0   2  0.400000  0.000000  0.000000   2.0   1   \n",
      "4   0    4    3  ...   1   3   2   5  0.333333  0.222222  1.500000   5.0   1   \n",
      "\n",
      "   bug-prone  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'bug-prone' column to 0 and 1\n",
    "data_transform['bug-prone'] = data_transform['bug-prone'].apply(lambda x: 1 if x.strip() == 'bug-prone' else 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data_transform.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split feature data and target data\n",
    "feature_X = data_transform.drop(columns=['Method name','bug-prone'])\n",
    "y = data_transform['bug-prone']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply SMOTETomek first (on original feature space)\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(feature_X, y)\n",
    "\n",
    "# Apply StandardScaler after resampling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "X = X_scaled\n",
    "y = y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "models = {\n",
    "    \n",
    "    \"RandomForest\" :  RandomForestClassifier(n_estimators=500, max_depth=None, min_samples_split=5, \n",
    "                                       min_samples_leaf=2, class_weight='balanced', random_state=42),\n",
    "    \"AdaBoostClassifier\" : AdaBoostClassifier(n_estimators=100, learning_rate=0.8, random_state=42),\n",
    "    \"BaggingClassifier\" : BaggingClassifier(n_estimators=100, max_samples=0.8, random_state=42),\n",
    "    \"KNeighborsClassifier\" : KNeighborsClassifier(n_neighbors=3, weights='distance', metric='manhattan'),\n",
    "    \"MLPClassifier\" : MLPClassifier(activation='relu', hidden_layer_sizes=(200,100), max_iter=2000, \n",
    "                               learning_rate='adaptive', random_state=42),\n",
    "    \"HistGradientBoostingClassifier\" : HistGradientBoostingClassifier(random_state=42),   \n",
    "    \"DecisionTreeClassifier\" : DecisionTreeClassifier(random_state=42),\n",
    "    \"SVC\" : SVC(random_state=42, probability=True, C=10, kernel='poly', gamma='scale'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=200, learning_rate=0.05, depth=4, verbose=0, random_state=42)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "feature_names = feature_X.columns\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# avg_probabilities = []\n",
    "# avg_scores = []\n",
    "\n",
    "# # Initialize cross-validation\n",
    "# kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Train all models and get probabilities\n",
    "# for name, model in models.items():\n",
    "#     print(name)\n",
    "#     probabilities = []\n",
    "#     max_scores = []\n",
    "\n",
    "#     for train_index, test_index in kf.split(X_df, y):\n",
    "#         X_train, X_test = X_df.iloc[train_index], X_df.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "        \n",
    "#         probabilities.append(y_pred_proba)\n",
    "#         max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "\n",
    "#     probabilities = np.concatenate(probabilities)  # Ensures consistent shape\n",
    "#     avg_probabilities.append(np.mean(probabilities))\n",
    "#     avg_scores.append(np.mean(max_scores))\n",
    "#     print(avg_probabilities)\n",
    "#     print(avg_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize AUC scores to get weights\n",
    "# weights = [score / sum(avg_scores) for score in avg_scores]\n",
    "# print(weights)\n",
    "\n",
    "# combined_probs = [weight * probs for weight, probs in zip(weights, y)]\n",
    "# print(len(combined_probs))\n",
    "\n",
    "# # Determine best threshold using precision-recall\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "#     f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "#     best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "#     return best_threshold\n",
    "\n",
    "# best_threshold_pr = get_best_threshold_precision_recall(y, combined_probs)\n",
    "# print(best_threshold_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import (\n",
    "#     make_scorer, precision_score, recall_score, f1_score, \n",
    "#     matthews_corrcoef, roc_auc_score\n",
    "# )\n",
    "# from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# # Define custom scorers for cross-validation\n",
    "# scoring = {\n",
    "#     'accuracy': 'accuracy',\n",
    "#     'precision': make_scorer(precision_score, average='binary'),  # For binary classification\n",
    "#     'recall': make_scorer(recall_score, average='binary'),\n",
    "#     'f1': make_scorer(f1_score, average='binary'),\n",
    "#     'mcc': make_scorer(matthews_corrcoef),\n",
    "#     'auc': make_scorer(roc_auc_score) # , needs_proba=True\n",
    "# }\n",
    "\n",
    "# # Helper function to display results\n",
    "# def print_cv_results(results):\n",
    "#     for metric in scoring.keys():\n",
    "#         mean = results[f'test_{metric}'].mean()\n",
    "#         std = results[f'test_{metric}'].std()\n",
    "#         print(f\"{metric.capitalize()}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "# def crossvalidate_fun(classifier, X_train, y_train):\n",
    "#     cv_results = cross_validate(classifier, X_train, y_train, cv=10, scoring=scoring)\n",
    "#     print_cv_results(cv_results)\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# # Train all models and get probabilities\n",
    "# for name, model in models.items():\n",
    "#     cv_results = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
    "#     scores.append(cv_results['test_auc'].mean())\n",
    "\n",
    "# print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and validation sets (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Combined Results:\n",
      "Accuracy: 0.8536\n",
      "Precision: 0.8436\n",
      "Recall: 0.8728\n",
      "F1-Score: 0.8580\n",
      "MCC: 0.7075\n",
      "AUC: 0.9375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probabilities = []\n",
    "max_scores = []\n",
    "\n",
    "# Train all models and get probabilities\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for class 1\n",
    "    probabilities.append(y_pred_proba)\n",
    "    max_scores.append(roc_auc_score(y_test, y_pred_proba))  # Use AUC as weight\n",
    "\n",
    "# Normalize AUC scores to get weights\n",
    "weights = [score / sum(max_scores) for score in max_scores]\n",
    "\n",
    "combined_probs = np.sum([weight * probs for weight, probs in zip(weights, probabilities)], axis=0)\n",
    "\n",
    "# Determine best threshold using precision-recall\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def get_best_threshold_precision_recall(y_true, y_proba):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))   \n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    return best_threshold\n",
    "\n",
    "best_threshold_pr = get_best_threshold_precision_recall(y_test, combined_probs)\n",
    "\n",
    "# Convert probabilities to binary predictions using threshold\n",
    "predictions = (combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "\n",
    "# Evaluate combined results\n",
    "print(\"\\nFinal Combined Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, predictions):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, predictions):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, predictions):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(y_test, predictions):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, combined_probs):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models, Weights, and Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all models\n",
    "for name, model in models.items():\n",
    "    with open(f\"Models/{name}_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save weights and threshold\n",
    "with open(\"weights_threshold.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"weights\": weights, \"best_threshold_pr\": best_threshold_pr}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & Use Later (Without Training Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "loaded_models = {}\n",
    "for name in models.keys():\n",
    "    with open(f\"Models/{name}_model.pkl\", \"rb\") as f:\n",
    "        loaded_models[name] = pickle.load(f)\n",
    "\n",
    "# Load weights and threshold\n",
    "with open(\"weights_threshold.pkl\", \"rb\") as f:\n",
    "    saved_data = pickle.load(f)\n",
    "    weights = saved_data[\"weights\"]\n",
    "    best_threshold_pr = saved_data[\"best_threshold_pr\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Method name  C20  C3  C4  C1  C5  C6  \\\n",
      "0        org.apache.zookeeper.graph.Log4JSource.init    8  52   5  42  10  33   \n",
      "1        org.apache.zookeeper.graph.Log4JSource.main    3  48   7  37  14  31   \n",
      "2  org.apache.zookeeper.graph.RandomAccessFileRea...    2  30   1  29   5  23   \n",
      "3  org.apache.zookeeper.graph.servlets.NumEvents....    3  35   6  29   8  24   \n",
      "4  org.apache.zookeeper.graph.servlets.StaticCont...    4  20   1  18   3  12   \n",
      "\n",
      "   C2  C21  C18  ...  H4  H1  H2  H3       H12  H13  H14  H15  H5  \\\n",
      "0   5   19   32  ...   1   1   0   1  0.019231  0.0  0.0  1.0   1   \n",
      "1   4    7    2  ...   1   2   0   2  0.041667  0.0  0.0  2.0   1   \n",
      "2   0    7   16  ...   1   1   0   1  0.033333  0.0  0.0  1.0   1   \n",
      "3   0   11   12  ...   1   1   0   1  0.028571  0.0  0.0  1.0   1   \n",
      "4   1   12    3  ...   1   3   0   3  0.150000  0.0  0.0  3.0   1   \n",
      "\n",
      "       bug-prone  \n",
      "0  not bug-prone  \n",
      "1      bug-prone  \n",
      "2      bug-prone  \n",
      "3      bug-prone  \n",
      "4      bug-prone  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import pandas libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "trdata = pd.read_csv(\"Test/zookeeper_result.csv\", delimiter=',')\n",
    "\n",
    "\n",
    "# Explore the dataset\n",
    "print(trdata.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split feature data and target data\n",
    "trdata_X = trdata.drop(columns=['Method name','bug-prone'])\n",
    "\n",
    "trdata_X_scaled = scaler.fit_transform(trdata_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_probabilities = []\n",
    "\n",
    "\n",
    "# Train all models and get probabilities\n",
    "for name, model in loaded_models.items():\n",
    "    y_pred_proba = model.predict_proba(trdata_X_scaled)[:, 1]  # Probability for class 1\n",
    "    tr_probabilities.append(y_pred_proba)\n",
    "\n",
    "\n",
    "tr_combined_probs = np.sum([weight * probs for weight, probs in zip(weights, tr_probabilities)], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Convert probabilities to binary predictions using threshold\n",
    "tr_predictions = (tr_combined_probs > best_threshold_pr).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Combined Results:\n",
      "Accuracy: 0.5944\n",
      "Precision: 0.5806\n",
      "Recall: 0.8793\n",
      "F1-Score: 0.6994\n",
      "MCC: 0.1837\n",
      "AUC: 0.7233\n"
     ]
    }
   ],
   "source": [
    "trdata_y = trdata['bug-prone'].apply(lambda x: 1 if x.strip() == 'bug-prone' else 0)\n",
    "\n",
    "# Evaluate combined results\n",
    "print(\"\\nFinal Combined Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(trdata_y, tr_predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(trdata_y, tr_predictions):.4f}\")\n",
    "print(f\"Recall: {recall_score(trdata_y, tr_predictions):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(trdata_y, tr_predictions):.4f}\")\n",
    "print(f\"MCC: {matthews_corrcoef(trdata_y, tr_predictions):.4f}\")\n",
    "print(f\"AUC: {roc_auc_score(trdata_y, tr_combined_probs):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy\n"
     ]
    }
   ],
   "source": [
    "index = 12\n",
    "\n",
    "if tr_predictions[index] == 1:\n",
    "    print(\"buggy\")\n",
    "else:\n",
    "    print(\"not buggy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
